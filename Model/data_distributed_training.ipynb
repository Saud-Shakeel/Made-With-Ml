{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_on</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-20 06:43:18</td>\n",
       "      <td>Comparison between YOLO and RCNN on real world...</td>\n",
       "      <td>Bringing theory to experiment is cool. We can ...</td>\n",
       "      <td>computer-vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-20 06:47:21</td>\n",
       "      <td>Show, Infer &amp; Tell: Contextual Inference for C...</td>\n",
       "      <td>The beauty of the work lies in the way it arch...</td>\n",
       "      <td>computer-vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>2020-02-24 16:24:45</td>\n",
       "      <td>Awesome Graph Classification</td>\n",
       "      <td>A collection of important graph embedding, cla...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>2020-02-28 23:55:26</td>\n",
       "      <td>Awesome Monte Carlo Tree Search</td>\n",
       "      <td>A curated list of Monte Carlo tree search pape...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>2020-03-07 23:04:31</td>\n",
       "      <td>AttentionWalk</td>\n",
       "      <td>A PyTorch Implementation of \"Watch Your Step: ...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           created_on                                              title   \n",
       "0   6  2020-02-20 06:43:18  Comparison between YOLO and RCNN on real world...  \\\n",
       "1   7  2020-02-20 06:47:21  Show, Infer & Tell: Contextual Inference for C...   \n",
       "2   9  2020-02-24 16:24:45                       Awesome Graph Classification   \n",
       "3  15  2020-02-28 23:55:26                    Awesome Monte Carlo Tree Search   \n",
       "4  25  2020-03-07 23:04:31                                      AttentionWalk   \n",
       "\n",
       "                                         description              tag  \n",
       "0  Bringing theory to experiment is cool. We can ...  computer-vision  \n",
       "1  The beauty of the work lies in the way it arch...  computer-vision  \n",
       "2  A collection of important graph embedding, cla...            other  \n",
       "3  A curated list of Monte Carlo tree search pape...            other  \n",
       "4  A PyTorch Implementation of \"Watch Your Step: ...            other  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_loc = 'dataset.csv'\n",
    "\n",
    "train_df = pd.read_csv(dataset_loc)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computer-vision', 'other', 'natural-language-processing', 'mlops']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique labels\n",
    "\n",
    "tags = train_df.tag.unique().tolist()\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_on</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>2020-03-03 13:54:31</td>\n",
       "      <td>Diffusion to Vector</td>\n",
       "      <td>Reference implementation of Diffusion2Vec (Com...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>2020-03-07 23:11:58</td>\n",
       "      <td>Graph Wavelet Neural Network</td>\n",
       "      <td>A PyTorch implementation of \"Graph Wavelet Neu...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>2020-03-08 00:32:58</td>\n",
       "      <td>Capsule Graph Neural Network</td>\n",
       "      <td>A PyTorch implementation of \"Capsule Graph Neu...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>2020-03-20 05:59:32</td>\n",
       "      <td>NeRF: Neural Radiance Fields</td>\n",
       "      <td>Representing scenes as neural radiance fields ...</td>\n",
       "      <td>computer-vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84</td>\n",
       "      <td>2020-03-20 15:18:43</td>\n",
       "      <td>Mention Classifier</td>\n",
       "      <td>Category prediction model\\r\\nThis repo contain...</td>\n",
       "      <td>natural-language-processing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           created_on                         title   \n",
       "0  19  2020-03-03 13:54:31           Diffusion to Vector  \\\n",
       "1  26  2020-03-07 23:11:58  Graph Wavelet Neural Network   \n",
       "2  44  2020-03-08 00:32:58  Capsule Graph Neural Network   \n",
       "3  80  2020-03-20 05:59:32  NeRF: Neural Radiance Fields   \n",
       "4  84  2020-03-20 15:18:43            Mention Classifier   \n",
       "\n",
       "                                         description   \n",
       "0  Reference implementation of Diffusion2Vec (Com...  \\\n",
       "1  A PyTorch implementation of \"Graph Wavelet Neu...   \n",
       "2  A PyTorch implementation of \"Capsule Graph Neu...   \n",
       "3  Representing scenes as neural radiance fields ...   \n",
       "4  Category prediction model\\r\\nThis repo contain...   \n",
       "\n",
       "                           tag  \n",
       "0                        other  \n",
       "1                        other  \n",
       "2                        other  \n",
       "3              computer-vision  \n",
       "4  natural-language-processing  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load inference dataset\n",
    "\n",
    "holdout_dataset = 'holdout.csv'\n",
    "\n",
    "test_df = pd.read_csv(holdout_dataset)\n",
    "test_df.head()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'D:\\MadeWithMl\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m      5\u001b[0m user_content \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhow are you\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# user content (message)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m client \u001b[39m=\u001b[39m OpenAI(\n\u001b[0;32m      8\u001b[0m     \u001b[39m# defaults to os.environ.get(\"OPENAI_API_KEY\")\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     api_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msk-qFWvmmJEUAKrh9mbx4d5T3BlbkFJk4LnaZ9z8r6qjzLILKZR\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m     13\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     14\u001b[0m     messages\u001b[39m=\u001b[39;49m[        \n\u001b[0;32m     15\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: system_content},\n\u001b[0;32m     16\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39massisstant\u001b[39;49m\u001b[39m\"\u001b[39;49m , \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: assistant_content},\n\u001b[0;32m     17\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: user_content},\n\u001b[0;32m     18\u001b[0m     ],    \n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent)\n",
      "File \u001b[1;32md:\\MadeWithMl\\venv\\lib\\site-packages\\openai\\_utils\\_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\MadeWithMl\\venv\\lib\\site-packages\\openai\\resources\\chat\\completions.py:594\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    549\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    550\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    592\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    593\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 594\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[0;32m    595\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    596\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[0;32m    597\u001b[0m             {\n\u001b[0;32m    598\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[0;32m    599\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[0;32m    600\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[0;32m    601\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[0;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[0;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[0;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[0;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[0;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[0;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[0;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[0;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[0;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[0;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[0;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[0;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[0;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[0;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[0;32m    616\u001b[0m             },\n\u001b[0;32m    617\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[0;32m    618\u001b[0m         ),\n\u001b[0;32m    619\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[0;32m    620\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    621\u001b[0m         ),\n\u001b[0;32m    622\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[0;32m    623\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    624\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[0;32m    625\u001b[0m     )\n",
      "File \u001b[1;32md:\\MadeWithMl\\venv\\lib\\site-packages\\openai\\_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[0;32m   1042\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1043\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1051\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m   1052\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[0;32m   1053\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[0;32m   1054\u001b[0m     )\n\u001b[1;32m-> 1055\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[1;32md:\\MadeWithMl\\venv\\lib\\site-packages\\openai\\_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    826\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    832\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    833\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    835\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    836\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    837\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    838\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    839\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[0;32m    840\u001b[0m     )\n",
      "File \u001b[1;32md:\\MadeWithMl\\venv\\lib\\site-packages\\openai\\_base_client.py:865\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mHTTPStatusError \u001b[39mas\u001b[39;00m err:  \u001b[39m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[0;32m    864\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m--> 865\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[0;32m    866\u001b[0m             options,\n\u001b[0;32m    867\u001b[0m             cast_to,\n\u001b[0;32m    868\u001b[0m             retries,\n\u001b[0;32m    869\u001b[0m             err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    870\u001b[0m             stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    871\u001b[0m             stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    872\u001b[0m         )\n\u001b[0;32m    874\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\MadeWithMl\\venv\\lib\\site-packages\\openai\\_base_client.py:925\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    923\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    926\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    927\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    928\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[0;32m    929\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    930\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    931\u001b[0m )\n",
      "File \u001b[1;32md:\\MadeWithMl\\venv\\lib\\site-packages\\openai\\_base_client.py:865\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mHTTPStatusError \u001b[39mas\u001b[39;00m err:  \u001b[39m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[0;32m    864\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m--> 865\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[0;32m    866\u001b[0m             options,\n\u001b[0;32m    867\u001b[0m             cast_to,\n\u001b[0;32m    868\u001b[0m             retries,\n\u001b[0;32m    869\u001b[0m             err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    870\u001b[0m             stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    871\u001b[0m             stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    872\u001b[0m         )\n\u001b[0;32m    874\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\MadeWithMl\\venv\\lib\\site-packages\\openai\\_base_client.py:925\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    923\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    926\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    927\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    928\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[0;32m    929\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    930\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    931\u001b[0m )\n",
      "File \u001b[1;32md:\\MadeWithMl\\venv\\lib\\site-packages\\openai\\_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m--> 877\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    879\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "system_content = \"you only answer in rhymes\"  # system content (behavior)\n",
    "assistant_content = \"\"  # assistant content (context)\n",
    "user_content = \"how are you\"  # user content (message)\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[        \n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"assisstant\" , \"content\": assistant_content},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ],    \n",
    ")\n",
    "print(response['choices'][0].message.content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to predict tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo-0613\"\n",
    "system_context = f\"\"\"\n",
    "    You are a NLP prediction service that predicts the label given an input's title and description.\n",
    "    You must choose between one of the following labels for each input: {tags}.\n",
    "    Only respond with the label name and nothing else.\n",
    "    \"\"\"\n",
    "assistant_content = \"\"\n",
    "user_context = \"Transfer learning with transformers: Using transformers for transfer learning on text classification tasks.\"\n",
    "\n",
    "def get_tag(model, system_content=\"\", assistant_content=\"\", user_content=\"\"):\n",
    "    try:\n",
    "\n",
    "        client = OpenAI(\n",
    "            # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "            api_key=\"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\",\n",
    "        )\n",
    "        # Get response from OpenAI\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[        \n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"assisstant\" , \"content\": assistant_content},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "        ],    \n",
    "        )\n",
    "        predicted_tag = response[\"choices\"][0].message.content\n",
    "        return predicted_tag\n",
    "\n",
    "    except (OpenAI.error.ServiceUnavailableError, OpenAI.error.APIError) as e:\n",
    "        return None\n",
    "    \n",
    "tag = get_tag(model=model, system_content=system_context, assistant_content=assistant_content, user_content=user_context)\n",
    "print (tag)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to predict tags for a list of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = test_df[[\"title\", \"description\"]].to_dict(orient=\"records\")[:3]\n",
    "\n",
    "def predict_tags(inputs, model, system_content='', assistant_content= ''):\n",
    "    y_pred = []\n",
    "\n",
    "    for item in tqdm(inputs):    \n",
    "    # Convert item dict to string\n",
    "        user_content = str(item)\n",
    "\n",
    "        # Get prediction\n",
    "        predicted_tag = get_tag(\n",
    "            model=model, system_content=system_content,\n",
    "            assistant_content=assistant_content, user_content=user_content)\n",
    "    \n",
    "    y_pred.append(predicted_tag)\n",
    "    return y_pred\n",
    "\n",
    "# Get predictions for a list of inputs\n",
    "predict_tags(inputs=samples, model=model, system_content=system_context)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to clean our predicted tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tags(y_pred, tags, default = 'other'):\n",
    "    for i, tag in enumerate(y_pred):\n",
    "\n",
    "        if tag not in tags:\n",
    "            y_pred[i] = default\n",
    "        if tag.startswith(\"'\") and tag.endswith(\"'\"):\n",
    "            y_pred[i] = tag[1:-1]\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tag_dist(y_true, y_pred):\n",
    "    # Distribution of tags\n",
    "    true_tag_freq = dict(Counter(y_true))\n",
    "    pred_tag_freq = dict(Counter(y_pred))\n",
    "    df_true = pd.DataFrame({\"tag\": list(true_tag_freq.keys()), \"freq\": list(true_tag_freq.values()), \"source\": \"true\"})\n",
    "    df_pred = pd.DataFrame({\"tag\": list(pred_tag_freq.keys()), \"freq\": list(pred_tag_freq.values()), \"source\": \"pred\"})\n",
    "    df = pd.concat([df_true, df_pred], ignore_index=True)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.title(\"Tag distribution\", fontsize=14)\n",
    "    ax = sns.barplot(x=\"tag\", y=\"freq\", hue=\"source\", data=df)\n",
    "    ax.set_xticklabels(list(true_tag_freq.keys()), rotation=0, fontsize=8)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to combine all the utiitites above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_df, model, system_content, tags, assistant_content=\"\"):\n",
    "    # Predictions\n",
    "    y_test = test_df.tag.to_list()\n",
    "    test_samples = test_df[[\"title\", \"description\"]].to_dict(orient=\"records\")\n",
    "    y_pred = predict_tags(\n",
    "        inputs=test_samples, model=model,\n",
    "        system_content=system_content, assistant_content=assistant_content)\n",
    "    y_pred = clean_tags(y_pred=y_pred, tags=tags)\n",
    "\n",
    "    # Performance\n",
    "    metrics = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "    performance = {\"precision\": metrics[0], \"recall\": metrics[1], \"f1\": metrics[2]}\n",
    "    print(json.dumps(performance, indent=2))\n",
    "    plot_tag_dist(y_true=y_test, y_pred=y_pred)\n",
    "    return y_pred, performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = {\"zero_shot\": {}, \"few_shot\": {}}\n",
    "performance = {\"zero_shot\": {}, \"few_shot\": {}}\n",
    "\n",
    "system_content = f\"\"\"\n",
    "    You are a NLP prediction service that predicts the label given an input's title and description.\n",
    "    You must choose between one of the following labels for each input: {tags}.\n",
    "    Only respond with the label name and nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "# Zero-shot with GPT 3.5\n",
    "\n",
    "method = \"zero_shot\"\n",
    "model = \"gpt-3.5-turbo-0613\"\n",
    "y_pred[method][model], performance[method][model] = evaluate(\n",
    "    test_df=test_df, model=model, system_content=system_content, tags=tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional context with few samples from each class\n",
    "\n",
    "num_samples = 2\n",
    "additional_context = []\n",
    "cols_to_keep = [\"title\", \"description\", \"tag\"]\n",
    "for tag in tags:\n",
    "    samples = train_df[cols_to_keep][train_df.tag == tag][:num_samples].to_dict(orient=\"records\")\n",
    "    additional_context.extend(samples)\n",
    "\n",
    "assistant_content = f\"\"\"Here are some examples with the correct labels: {additional_context}\"\"\"\n",
    "\n",
    "# Few-shot with GPT 3.5\n",
    "\n",
    "method = \"few_shot\"\n",
    "model = \"gpt-3.5-turbo-0613\"\n",
    "y_pred[method][model], performance[method][model] = evaluate(\n",
    "    test_df=test_df, model=model, system_content=system_content,\n",
    "    assistant_content=assistant_content, tags=tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from ray.data.preprocessor import Preprocessor\n",
    "import numpy as np\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    eval(\"setattr(torch.backends.cudnn, 'deterministic', True)\")\n",
    "    eval(\"setattr(torch.backends.cudnn, 'benchmark', False)\")\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(num_samples=None):\n",
    "    ds = ray.data.read_csv('dataset.csv')\n",
    "    ds = ds.random_shuffle(seed=1234)\n",
    "    ds = ray.data.from_items(ds.take(num_samples) if num_samples else ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from madewithml.data import preprocess\n",
    "\n",
    "class customPreprocessor(Preprocessor):\n",
    "    def _fit(self, ds):\n",
    "        tags = ds.unique(column='tag')\n",
    "        self.class_to_index = {tag:i for i, tag in enumerate(tags)}\n",
    "        self.index_to_class = {k:v for v, k in self.class_to_index.items()}\n",
    "    \n",
    "    def _transform_pandas(self, batch):\n",
    "        return preprocess(batch, class_to_index=self.class_to_index)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "llm = BertModel.from_pretrained('allenai/scibert_scivocab_uncased', return_dict= False)\n",
    "embedding_dim = llm.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', returned_dict = False)\n",
    "\n",
    "# Sample\n",
    "\n",
    "text = \"Transfer learning with transformers for text classification.\"\n",
    "batch = tokenizer([text], return_tensors=\"np\", padding=\"longest\")\n",
    "batch = {k:torch.tensor(v) for k,v in batch.items()}  # convert to torch tensors\n",
    "seq, pool = llm(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fineTunedLLM(nn.Module):\n",
    "    def __init__(self, llm, num_classes, embedding_dim, drop_out):\n",
    "        super(fineTunedLLM, self).__init__()\n",
    "        self.llm = llm\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        # Use the length of num_classes as the number of output features\n",
    "        self.fc1 = nn.Linear(embedding_dim, len(num_classes))\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        ids, mask = batch['ids'], batch['mask']\n",
    "        seq, pool = self.llm(input_ids=ids, attention_mask=mask)\n",
    "        z = self.dropout(pool)\n",
    "        z = self.fc1(z)\n",
    "        return z\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict(self, batch):\n",
    "        self.eval()\n",
    "        logits = self(batch)\n",
    "        y_pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        return y_pred\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict_proba(self, batch):\n",
    "        self.eval()\n",
    "        logits = self(batch)\n",
    "        proba = torch.softmax(logits).cpu().numpy()\n",
    "        return proba\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_parameters of fineTunedLLM(\n",
      "  (llm): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=768, out_features=4, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "\n",
    "num_classes = train_df.tag.unique().tolist()\n",
    "model = fineTunedLLM(llm=llm, drop_out=0.5, embedding_dim=embedding_dim, num_classes=num_classes)\n",
    "print(model.named_parameters)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import get_device\n",
    "\n",
    "def pad_array(arr, dtype = np.int32):\n",
    "    max_len = max(len(row) for row in arr)\n",
    "    pad_array = np.zeros((arr.shape[0], max_len), dtype=dtype)\n",
    "\n",
    "    for i, row in enumerate(arr):\n",
    "        pad_array[i][:len(row)] = row\n",
    "        return pad_array\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch['ids'] = pad_array(batch['ids'])\n",
    "    batch['masks'] = pad_array(batch['masks'])\n",
    "    dtypes = {'ids': torch.int32, 'masks': torch.int32, 'targets': torch.int64}\n",
    "    tensor_batch = {}\n",
    "\n",
    "    for key, array in batch.items():\n",
    "        tensor_batch[key] = torch.as_tensor(array, dtype=dtypes[key], device=get_device())\n",
    "    return tensor_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001660287AD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Failed to detect number of TPUs: [WinError 3] The system cannot find the path specified: '/dev/vfio'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 14:09:24,835\tINFO worker.py:1633 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-11-13 14:09:35,371\tINFO read_api.py:406 -- To satisfy the requested parallelism of 8, each read task output is split into 8 smaller blocks.\n",
      "2023-11-13 14:09:35,443\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV->SplitBlocks(8)] -> AllToAllOperator[RandomShuffle] -> LimitOperator[limit=1]\n",
      "2023-11-13 14:09:35,443\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-11-13 14:09:35,451\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d58900fd4934583a19637681a0a7f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- RandomShuffle 1:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06edc9cdf69c44dfad9adf1b82819203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 2:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a5244097c1417488dd9f1cc8969b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 3:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f293e470b89341abb85977b3f6b9e58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 14:09:41,939\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV->SplitBlocks(8)] -> AllToAllOperator[RandomShuffle] -> AllToAllOperator[Sort] -> AllToAllOperator[MapBatches(group_fn)->MapBatches(_filter_split)->RandomShuffle] -> LimitOperator[limit=1]\n",
      "2023-11-13 14:09:41,947\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=True, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-11-13 14:09:41,947\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a638d7a6f4bc4d118827120ed97563de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- RandomShuffle 1:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe6bc02042143d4963dccfe67a641de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 2:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f494234209c247e5adc96a9aa35a7db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 3:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856bb9b2dcf54a25b3b76dcb18860847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Sort 4:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8433b01aae9149f69ee11567563b814e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 5:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcd775cf1834cf79fda41c1f840f508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 6:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b366a3d52654970a66f0b99a9b5aa8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 7:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9931ede1528d460dafeb980b5f098998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(group_fn)->MapBatches(_filter_split)->RandomShuffle 8:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a538f866f749c7958f35ae39320e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 9:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39654b90669442ebb33af3b047bea15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 10:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ef0511766643058e7864eb374c8a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adf20d326324260870a3cebd82d5146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 0:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 14:09:58,215\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV->SplitBlocks(8)] -> AllToAllOperator[RandomShuffle] -> AllToAllOperator[Sort] -> AllToAllOperator[MapBatches(group_fn)->MapBatches(_filter_split)->RandomShuffle] -> AllToAllOperator[Aggregate] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "2023-11-13 14:09:58,223\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=True, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-11-13 14:09:58,223\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b739044174c49ed858498798e772b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- RandomShuffle 1:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba6d78640dd47b3b27f6237c1b474cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 2:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23aa6a57052843e79f77d2d8dbf52465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 3:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e7f71a8da04cf8a65fb1e4072cad93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Sort 4:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a4f55a77f94dfbaab1b7782c5500af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 5:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fffcae2adff4c29a2cbb7d7481be379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 6:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc307dbde93489991aee72541b8c23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 7:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bf9c95341c409bbe61a132c261280a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(group_fn)->MapBatches(_filter_split)->RandomShuffle 8:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902f65d2eb7c4eb082c0dcb964e3b047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 9:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fb29ab00994bc48fd638a16ec4f8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 10:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec59882cc5ac4be4b22dea201226f2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Aggregate 11:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2bfcabe6da4315aacb16e11bad6389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 12:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d409cbcc630849838dc33a1ff0601abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 13:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ba469638ae4434972dce715eb21cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49f65d123a740aca0737d5c7b32e8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 0:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5732e9b3f85140358366dacf182dbe40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 0:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 14:10:02,962\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV->SplitBlocks(8)] -> AllToAllOperator[RandomShuffle] -> AllToAllOperator[Sort] -> AllToAllOperator[MapBatches(group_fn)->MapBatches(_filter_split)->RandomShuffle] -> TaskPoolMapOperator[MapBatches(preprocess)] -> LimitOperator[limit=128]\n",
      "2023-11-13 14:10:02,970\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=True, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-11-13 14:10:02,970\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6326fa424aef43e3b5f0c1e5f7615083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- RandomShuffle 1:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adffea7646594668bea63753e6df3d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 2:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58ecc1e821b4f4b88ba6a05370e057b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 3:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514679622b714b88b01319b5be4e323f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Sort 4:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c171ddef4f4436e936a84aee4ee79bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 5:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efed756755148d78ec7aa14e3ecc59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 6:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6bb4519bc84792812b4d2eeaa28b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 7:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e598eeb5be0f49e98e7baf674f5a58a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(group_fn)->MapBatches(_filter_split)->RandomShuffle 8:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b514adfbe74c93a66c2fd0d44e63d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 9:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a884a399bbb42e99dd73540a4b11bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 10:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bee44502cf74109a392f6803ec5d5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b1edfcc5c34d809f2d3754ab1dca36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 0:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saud\\AppData\\Local\\Temp\\ipykernel_12264\\4154227819.py:18: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  tensor_batch[key] = torch.as_tensor(array, dtype=dtypes[key], device=get_device())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([[  102,  2848, 11695,  ...,     0,     0,     0],\n",
       "         [    0,     0,     0,  ...,     0,     0,     0],\n",
       "         [    0,     0,     0,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    0,     0,     0,  ...,     0,     0,     0],\n",
       "         [    0,     0,     0,  ...,     0,     0,     0],\n",
       "         [    0,     0,     0,  ...,     0,     0,     0]], dtype=torch.int32),\n",
       " 'masks': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32),\n",
       " 'targets': tensor([2, 3, 3, 2, 0, 0, 2, 0, 1, 3, 0, 2, 0, 2, 1, 2, 2, 2, 3, 2, 0, 0, 2, 0,\n",
       "         0, 2, 2, 0, 3, 2, 0, 1, 1, 0, 2, 1, 0, 3, 2, 0, 0, 0, 0, 3, 2, 0, 2, 0,\n",
       "         0, 2, 0, 2, 2, 0, 0, 2, 0, 3, 1, 2, 0, 2, 2, 2, 3, 2, 3, 3, 2, 1, 0, 0,\n",
       "         2, 0, 0, 2, 2, 2, 2, 3, 3, 2, 0, 0, 2, 0, 1, 3, 0, 2, 0, 2, 1, 2, 2, 2,\n",
       "         3, 2, 0, 0, 2, 0, 0, 2, 2, 0, 3, 2, 0, 1, 1, 0, 2, 1, 0, 3, 2, 0, 0, 0,\n",
       "         0, 3, 2, 0, 2, 0, 0, 2])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from madewithml.data import stratify_split\n",
    "\n",
    "ds = ray.data.read_csv('dataset.csv')\n",
    "ds = ds.random_shuffle(seed=42)\n",
    "\n",
    "\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "train_ds, val_ds = stratify_split(ds, stratify='tag', test_size= TEST_SIZE)\n",
    "# Mapping\n",
    "tags = train_ds.unique(column='tag')\n",
    "class_to_idx = {tag:i for i, tag in enumerate(tags)}\n",
    "\n",
    "sample_ds = train_ds.map_batches(preprocess, fn_kwargs={\"class_to_index\": class_to_idx}, batch_format=\"pandas\")\n",
    "\n",
    "sample_batch = sample_ds.take_batch(batch_size=128)\n",
    "collate_fn(sample_batch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilitites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air import Checkpoint, session\n",
    "from ray.air.config import CheckpointConfig, DatasetConfig, RunConfig, ScalingConfig\n",
    "import ray.train as train\n",
    "from ray.train.torch import TorchCheckpoint, TorchTrainer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(ds, batch_size, model, num_classes, loss_fn, optimizer):\n",
    "    \"\"\"Train step.\"\"\"\n",
    "    model.train()\n",
    "    loss = 0.0\n",
    "    ds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=collate_fn)\n",
    "    for i, batch in enumerate(ds_generator):\n",
    "        optimizer.zero_grad()  # reset gradients\n",
    "        z = model(batch)  # forward pass\n",
    "        targets = F.one_hot(batch[\"targets\"], num_classes=num_classes).float()  # one-hot (for loss_fn)\n",
    "        J = loss_fn(z, targets)  # define loss\n",
    "        J.backward()  # backward pass\n",
    "        optimizer.step()  # update weights\n",
    "        loss += (J.detach().item() - loss) / (i + 1)  # cumulative loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(ds, batch_size, model, num_classes, loss_fn):\n",
    "    \"\"\"Eval step.\"\"\"\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    y_trues, y_preds = [], []\n",
    "    ds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=collate_fn)\n",
    "    with torch.inference_mode():\n",
    "        for i, batch in enumerate(ds_generator):\n",
    "            z = model(batch)\n",
    "            targets = F.one_hot(batch[\"targets\"], num_classes=num_classes).float()  # one-hot (for loss_fn)\n",
    "            J = loss_fn(z, targets).item()\n",
    "            loss += (J - loss) / (i + 1)\n",
    "            y_trues.extend(batch[\"targets\"].cpu().numpy())\n",
    "            y_preds.extend(torch.argmax(z, dim=1).cpu().numpy())\n",
    "    return loss, np.vstack(y_trues), np.vstack(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_loop_per_worker(config):\n",
    "    # Hyperparameters\n",
    "    dropout_p = config[\"dropout_p\"]\n",
    "    lr = config[\"lr\"]\n",
    "    lr_factor = config[\"lr_factor\"]\n",
    "    lr_patience = config[\"lr_patience\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_classes = config[\"num_classes\"]\n",
    "\n",
    "    # Get datasets\n",
    "    set_seed()\n",
    "    train_ds = session.get_dataset_shard(\"train\")\n",
    "    val_ds = session.get_dataset_shard(\"val\")\n",
    "\n",
    "    # Model\n",
    "    llm = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "    model = fineTunedLLM(llm=llm, dropout_p=dropout_p, embedding_dim=llm.config.hidden_size, num_classes=num_classes)\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    # Training components\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=lr_factor, patience=lr_patience)\n",
    "\n",
    "    # Training\n",
    "    batch_size_per_worker = batch_size // session.get_world_size()\n",
    "    for epoch in range(num_epochs):\n",
    "        # Step\n",
    "        train_loss = train_step(train_ds, batch_size_per_worker, model, num_classes, loss_fn, optimizer)\n",
    "        val_loss, _, _ = eval_step(val_ds, batch_size_per_worker, model, num_classes, loss_fn)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Checkpoint\n",
    "        metrics = dict(epoch=epoch, lr=optimizer.param_groups[0][\"lr\"], train_loss=train_loss, val_loss=val_loss)\n",
    "        checkpoint = TorchCheckpoint.from_model(model=model)\n",
    "        session.report(metrics, checkpoint=checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_workers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 14\u001b[0m\n\u001b[0;32m      2\u001b[0m train_loop_config \u001b[39m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdropout_p\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.5\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1e-4\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnum_classes\u001b[39m\u001b[39m\"\u001b[39m: num_classes,\n\u001b[0;32m     10\u001b[0m }\n\u001b[0;32m     12\u001b[0m \u001b[39m# Scaling config\u001b[39;00m\n\u001b[0;32m     13\u001b[0m scaling_config \u001b[39m=\u001b[39m ScalingConfig(\n\u001b[1;32m---> 14\u001b[0m     num_workers\u001b[39m=\u001b[39mnum_workers,\n\u001b[0;32m     15\u001b[0m     use_gpu\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(resources_per_worker[\u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m     16\u001b[0m     resources_per_worker\u001b[39m=\u001b[39mresources_per_worker,\n\u001b[0;32m     17\u001b[0m     _max_cpu_fraction_per_node\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[39m# Run config\u001b[39;00m\n\u001b[0;32m     21\u001b[0m checkpoint_config \u001b[39m=\u001b[39m CheckpointConfig(num_to_keep\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, checkpoint_score_attribute\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m, checkpoint_score_order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_workers' is not defined"
     ]
    }
   ],
   "source": [
    "# Train loop config\n",
    "train_loop_config = {\n",
    "    \"dropout_p\": 0.5,\n",
    "    \"lr\": 1e-4,\n",
    "    \"lr_factor\": 0.8,\n",
    "    \"lr_patience\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_classes\": num_classes,\n",
    "}\n",
    "\n",
    "# Scaling config\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=num_workers,\n",
    "    use_gpu=bool(resources_per_worker[\"GPU\"]),\n",
    "    resources_per_worker=resources_per_worker,\n",
    "    _max_cpu_fraction_per_node=0.8,\n",
    ")\n",
    "\n",
    "# Run config\n",
    "checkpoint_config = CheckpointConfig(num_to_keep=1, checkpoint_score_attribute=\"val_loss\", checkpoint_score_order=\"min\")\n",
    "run_config = RunConfig(name=\"llm\", checkpoint_config=checkpoint_config, local_dir=\"~/ray_results\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 14:21:54,427\tINFO read_api.py:406 -- To satisfy the requested parallelism of 8, each read task output is split into 8 smaller blocks.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Use `ds.count()` to compute the length of a distributed Dataset. This may be an expensive operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load and split data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ds \u001b[39m=\u001b[39m load_data()\n\u001b[0;32m      3\u001b[0m train_ds, val_ds \u001b[39m=\u001b[39m stratify_split(ds, stratify\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtag\u001b[39m\u001b[39m\"\u001b[39m, test_size\u001b[39m=\u001b[39mTEST_SIZE)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Preprocess\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(num_samples)\u001b[0m\n\u001b[0;32m      2\u001b[0m ds \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdataset.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mrandom_shuffle(seed\u001b[39m=\u001b[39m\u001b[39m1234\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m ds \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfrom_items(ds\u001b[39m.\u001b[39;49mtake(num_samples) \u001b[39mif\u001b[39;49;00m num_samples \u001b[39melse\u001b[39;49;00m ds)\n\u001b[0;32m      5\u001b[0m \u001b[39mreturn\u001b[39;00m ds\n",
      "File \u001b[1;32md:\\MadeWithMl\\venv\\lib\\site-packages\\ray\\data\\read_api.py:150\u001b[0m, in \u001b[0;36mfrom_items\u001b[1;34m(items, parallelism)\u001b[0m\n\u001b[0;32m    144\u001b[0m detected_parallelism, _, _ \u001b[39m=\u001b[39m _autodetect_parallelism(\n\u001b[0;32m    145\u001b[0m     parallelism,\n\u001b[0;32m    146\u001b[0m     ray\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mget_current_placement_group(),\n\u001b[0;32m    147\u001b[0m     DataContext\u001b[39m.\u001b[39mget_current(),\n\u001b[0;32m    148\u001b[0m )\n\u001b[0;32m    149\u001b[0m \u001b[39m# Truncate parallelism to number of items to avoid empty blocks.\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m detected_parallelism \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mlen\u001b[39;49m(items), detected_parallelism)\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m detected_parallelism \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    153\u001b[0m     block_size, remainder \u001b[39m=\u001b[39m \u001b[39mdivmod\u001b[39m(\u001b[39mlen\u001b[39m(items), detected_parallelism)\n",
      "File \u001b[1;32md:\\MadeWithMl\\venv\\lib\\site-packages\\ray\\data\\dataset.py:5151\u001b[0m, in \u001b[0;36mDataset.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m-> 5151\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m   5152\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse `ds.count()` to compute the length of a distributed Dataset. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   5153\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis may be an expensive operation.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   5154\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: Use `ds.count()` to compute the length of a distributed Dataset. This may be an expensive operation."
     ]
    }
   ],
   "source": [
    "# Load and split data\n",
    "ds = load_data()\n",
    "train_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=TEST_SIZE)\n",
    "\n",
    "# Preprocess\n",
    "preprocessor = customPreprocessor()\n",
    "train_ds =  preprocessor.fit_transform(train_ds)\n",
    "val_ds = preprocessor.transform(val_ds)\n",
    "train_ds = train_ds.materialize()\n",
    "val_ds = val_ds.materialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset config\n",
    "dataset_config = {\n",
    "    \"train\": DatasetConfig(fit=False, transform=False, randomize_block_order=False),\n",
    "    \"val\": DatasetConfig(fit=False, transform=False, randomize_block_order=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
    "    dataset_config=dataset_config,\n",
    "    preprocessor=preprocessor,\n",
    ")\n",
    "\n",
    "# Train\n",
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.metrics_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.best_checkpoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchPredictor\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor\n",
    "best_checkpoint = results.best_checkpoints[0][0]\n",
    "predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
    "preprocessor = predictor.get_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test (holdout) dataset\n",
    "HOLDOUT_LOC = 'holdout.csv'\n",
    "test_ds = ray.data.read_csv(HOLDOUT_LOC)\n",
    "preprocessed_ds = preprocessor.transform(test_ds)\n",
    "preprocessed_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true\n",
    "values = preprocessed_ds.select_columns(cols=[\"targets\"]).take_all()\n",
    "y_true = np.stack([item[\"targets\"] for item in values])\n",
    "print (y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred\n",
    "z = predictor.predict(data=test_ds.to_pandas())[\"predictions\"]\n",
    "y_pred = np.stack(z).argmax(1)\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "{\"precision\": metrics[0], \"recall\": metrics[1], \"f1\": metrics[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ds, predictor):\n",
    "    # y_true\n",
    "    preprocessor = predictor.get_preprocessor()\n",
    "    preprocessed_ds = preprocessor.transform(ds)\n",
    "    values = preprocessed_ds.select_columns(cols=[\"targets\"]).take_all()\n",
    "    y_true = np.stack([item[\"targets\"] for item in values])\n",
    "\n",
    "    # y_pred\n",
    "    z = predictor.predict(data=ds.to_pandas())[\"predictions\"]\n",
    "    y_pred = np.stack(z).argmax(1)\n",
    "\n",
    "    # Evaluate\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    performance = {\"precision\": metrics[0], \"recall\": metrics[1], \"f1\": metrics[2]}\n",
    "    return performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_prob(prob, index_to_class):\n",
    "    d = {}\n",
    "    for i, item in enumerate(prob):\n",
    "        d[index_to_class[i]] = item\n",
    "    return d\n",
    "\n",
    "def decode(indices, idx_to_label):\n",
    "    return [idx_to_label[index] for index in indices]\n",
    "\n",
    "\n",
    "def predict_with_proba(df, predictor):\n",
    "    preprocessor = predictor.get_preprocessor()\n",
    "    z = predictor.predict(data=df)[\"predictions\"]\n",
    "    y_prob = torch.tensor(np.stack(z)).softmax(dim=1).numpy()\n",
    "    results = []\n",
    "    for i, prob in enumerate(y_prob):\n",
    "        tag = decode([z[i].argmax()], preprocessor.index_to_class)[0]\n",
    "        results.append({\"prediction\": tag, \"probabilities\": format_prob(prob, preprocessor.index_to_class)})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor\n",
    "predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
    "preprocessor = predictor.get_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on sample\n",
    "title = \"Transfer learning with transformers\"\n",
    "description = \"Using transformers for transfer learning on text classification tasks.\"\n",
    "sample_df = pd.DataFrame([{\"title\": title, \"description\": description, \"tag\": \"other\"}])\n",
    "predict_with_proba(df=sample_df, predictor=predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
